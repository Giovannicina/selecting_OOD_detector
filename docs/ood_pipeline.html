<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>selecting_OOD_detector.pipeline.ood_pipeline API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>selecting_OOD_detector.pipeline.ood_pipeline</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from collections import defaultdict
from typing import Optional

import pandas as pd
from selecting_OOD_detector.pipeline.base import BasePipeline
from selecting_OOD_detector.utils.scores_metrics import (score_dataset,
                                                         get_ood_aucs_score_for_all_models,
                                                         average_values_in_nested_dict,
                                                         get_mean_stderr_annots_in_nested_dict
                                                         )
from selecting_OOD_detector.utils.general import check_and_convert_dfs_to_numpy
from selecting_OOD_detector.utils.plotting import plot_heatmap, plot_scores_boxplot, plot_scores_distr
from selecting_OOD_detector.models.novelty_estimators_info import SCORING_FUNCTIONS


class OODPipeline(BasePipeline):
    &#34;&#34;&#34;
    Pipeline to fit novelty estimators on in-distribution data and evaluate novelty of Out-of-Distribution (OOD)
    groups.


    Example of usage:

        # Initialize the pipeline
        oodpipeline = OODPipeline()

        # Fit the pipeline on in-distribution training data and compute novelty scores for in-distribution test data
        oodpipeline.fit(X_train= X_train, X_test=X_test)

        # Define OOD groups and evaluate by the pipeline
        ood_groups = {&#34;Flu patients&#34;: X_flu, &#34;Ventilated patients&#34;: X_vent}
        oodpipeline.evaluate(ood_groups)

        # Inspect AUC-ROC scores of detecting OOD groups
        oodpipeline.get_ood_aucs_scores()
    &#34;&#34;&#34;

    def __init__(self,
                 **kwargs):
        &#34;&#34;&#34;

        Parameters
        ----------
        kwargs
            model_selection: set
                Define which models to train, e.g. {&#34;PPCA&#34;, &#34;LOF&#34;, &#34;VAE&#34;}. If selection is not provided, all available
                models are used.
        &#34;&#34;&#34;
        super().__init__(**kwargs)
        self.in_domain_scores = defaultdict(dict)
        self.out_domain_scores = defaultdict(dict)
        self.feature_names = None

    def fit(self,
            X_train,
            X_test,
            **kwargs):
        &#34;&#34;&#34;
        Fits models on training data with n_trials different runs.  Novelty estimators from each run are stored
        in a nested dictionary in self.novelty_estimators.
        (E.g.: {0: {&#34;AE&#34;: NoveltyEstimator, &#34;PPCA&#34;: NoveltyEstimator},
               1: {&#34;AE&#34;: NoveltyEstimator, &#34;PPCA&#34;: NoveltyEstimator}} )
        Parameters
        ----------
        X_train: pd.DataFrame
            Training in-distribution data. Used to fit novelty estimators.
        X_test: pd.DataFrame
            Test in-distribution data. Used to calculate self.in_domain_scores which are taken as base novelty scores
            for the dataset and used for comparison against OOD groups later.
        kwargs:
            y_train: pd.DataFrame
                Labels corresponding to training data.
            n_trials: int
                Number of trials to run.
        &#34;&#34;&#34;
        y_train = kwargs.get(&#34;y_train&#34;, None)
        n_trials = kwargs.get(&#34;n_trials&#34;, 5)

        assert list(X_train.columns) == list(X_test.columns), &#34;Train and test data have different features!&#34;
        self.feature_names = list(X_train.columns)

        print(&#34;--- OOD Pipeline ---&#34;)
        print(&#34;1/2 Fitting novelty estimators...&#34;)
        self._fit(X_train=X_train, y_train=y_train, n_trials=n_trials)

        print(&#34;2/2 Scoring in-domain data...&#34;)
        self.in_domain_scores = self._score_in_domain(X_test)

    def evaluate_ood_groups(self,
                            ood_groups: dict,
                            return_averaged: bool = False):
        &#34;&#34;&#34;
        Gives novelty scores to OOD groups.
        Returns and stores dictionary of novelty scores given by each model for each sample in every OOD group.
        If the function is called repeadetly, updates internally stored novelty scores for the OOD groups.

        Parameters
        ----------
        ood_groups: dict
            Dictionary of OOD groups. Dictionary has to contain a name of each OOD group and features in a pd.DataFrame.
            Example: {&#34;Flu patients&#34;: X_flu, &#34;Ventilated patients&#34;: X_vent}

        return_averaged: bool
            If true, returns averaged novelty score for each sample. The shape of novelty scores given by each model
            then corresponds to (1, n_samples).
            Else, the shape of novelty scores given by each model is (n_trials, n_samples) where n_trials is
            the number of trials used in the fit function.

        Returns
        -------
        out_domain_scores: dict
            Returns a dictionary of novelty scores given by each model for each sample in every OOD group.

        &#34;&#34;&#34;
        assert self.novelty_estimators, &#34;Novelty estimator dictionary is empty.&#34; \
                                        &#34;Please fit novelty estimators to in-distribution data before calling&#34; \
                                        &#34;evaluate_ood_groups.&#34;

        assert all([list(X_ood.columns) == self.feature_names for _, X_ood in ood_groups.items()]), \
            &#34;All OOD groups must have identical features to in-distribution data!&#34;

        out_domain_scores = self._score_out_domain(ood_groups)
        self.out_domain_scores.update(out_domain_scores)

        if return_averaged:
            return average_values_in_nested_dict(out_domain_scores)

        return out_domain_scores

    def get_auc_scores(self,
                       ood_groups_selections: Optional[list] = None,
                       return_averaged: bool = True):
        &#34;&#34;&#34;
        Computes AUC-ROC scores of OOD detection for each OOD group as compared to the in-distribution test data.
        By default, returns scores for every group evaluated by the pipeline (evaluate_ood_groups).

        Parameters
        ----------
        ood_groups_selections: Optional(list)
            Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
            is provided, all groups ever evaluate by the pipeline will be included.
        return_averaged: bool
            Indicates whether to return averaged AUC-ROC scores over n_trials run or a list of scores for every trial.

        Returns
        -------
        aucs_dict_groups: dict
            A nested dictionary that contains a name of OOD group, name of novelty estimator and either a float (if
            averaged) or a list of AUC-ROC scores.

        &#34;&#34;&#34;
        selected_ood_group = self._filter_ood_groups(ood_groups_selections)
        aucs_dict_groups = defaultdict(lambda: defaultdict(list))

        for ood_group_name in selected_ood_group:
            aucs_dict_groups[ood_group_name] = get_ood_aucs_score_for_all_models(
                ood_scores_trials_dict=self.out_domain_scores[ood_group_name],
                test_scores_trials_dict=self.in_domain_scores,
            )

        if return_averaged:
            return average_values_in_nested_dict(aucs_dict_groups)

        return aucs_dict_groups

    def plot_auc_scores(self,
                        ood_groups_selections: Optional[list] = None,
                        show_stderr: bool = True,
                        save_dir: str = None,
                        **plot_kwargs):
        &#34;&#34;&#34;
        Plots a heatmap of AUC-ROC scores of OOD detection for each OOD group as compared to
        the in-distribution test data.

        Parameters
        ----------
        ood_groups_selections: Optional(list)
            Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
            is provided, all groups ever evaluate by the pipeline will be included.
        show_stderr: Optional(bool)
            If True (default), annotates the heatmpa with means and standard error (calculated using jacknife
            resampling). Else, plots the mean values only.
        save_dir: Optional(str)
            If a path to a directory is provided, saves plots for each OOD group separately.
        plot_kwargs:
            Other arguments to be passed to sns.heatmap function.
        &#34;&#34;&#34;

        auc_scores = self.get_auc_scores(ood_groups_selections=ood_groups_selections,
                                         return_averaged=show_stderr)
        if not show_stderr:
            annots = get_mean_stderr_annots_in_nested_dict(auc_scores)
            annots = pd.DataFrame(annots).values.T
            auc_scores = average_values_in_nested_dict(auc_scores)
            plot_fmt = &#34;s&#34;

        else:
            annots = True
            plot_fmt = &#34;.2g&#34;

        plot_df = pd.DataFrame(auc_scores)

        plot_heatmap(plot_df,
                     title=&#34;AUC&#34;,
                     save_dir=save_dir,
                     annot=annots,
                     fmt=plot_fmt,
                     annot_kws={&#34;fontsize&#34;: 9},
                     figsize=(12, 0.75 * len(plot_df.columns)),
                     **plot_kwargs,
                     )

    def plot_score_distr(self,
                         ood_groups_selections: Optional[list] = None,
                         save_dir=None
                         ):
        &#34;&#34;&#34;

        Parameters
        ----------
        ood_groups_selections: Optional(list)
            Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
            is provided, all groups ever evaluate by the pipeline will be included.
        save_dir: Optional(str)
            If a path to a directory is provided, saves plots for each OOD group separately.
        &#34;&#34;&#34;
        out_domain_scores_mean = average_values_in_nested_dict(self.out_domain_scores, axis=0, dict_level=2)
        in_domain_scores_mean = average_values_in_nested_dict(self.in_domain_scores, axis=0, dict_level=1)

        selected_ood_group = self._filter_ood_groups(ood_groups_selections)

        for ood_name in selected_ood_group:
            ood_scores = out_domain_scores_mean[ood_name]
            save_group_name = ood_name.lower().replace(&#34; &#34;, &#34;_&#34;)

            if save_dir is not None:
                save_dir_ = f&#34;{save_dir}_{save_group_name}.png&#34;
            else:
                save_dir_ = None

            plot_scores_distr(scores_test=in_domain_scores_mean,
                              scores_new=ood_scores,
                              title=ood_name,
                              clip_q=0.05,
                              kind=&#34;hist&#34;,
                              bins=30,
                              save_dir=save_dir_,
                              labels=SCORING_FUNCTIONS)

    def plot_box_plot(self,
                      ood_groups_selections: Optional[list] = None,
                      save_dir=None
                      ):
        &#34;&#34;&#34;

        Parameters
        ----------
        ood_groups_selections: Optional(list)
            Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
            is provided, all groups ever evaluate by the pipeline will be included.
        save_dir: Optional(str)
            If a path to a directory is provided, saves plots for each OOD group separately.
        &#34;&#34;&#34;

        out_domain_scores_mean = average_values_in_nested_dict(self.out_domain_scores, axis=0, dict_level=2)
        in_domain_scores_mean = average_values_in_nested_dict(self.in_domain_scores, axis=0, dict_level=1)

        selected_ood_group = self._filter_ood_groups(ood_groups_selections)

        for ood_name in selected_ood_group:
            ood_scores = out_domain_scores_mean[ood_name]

            if save_dir is not None:
                save_dir_ = f&#34;{save_dir}_{ood_name}.png&#34;
            else:
                save_dir_ = None

            plot_scores_boxplot(scores_test=in_domain_scores_mean,
                                scores_new=ood_scores,
                                show_outliers=True,
                                title=ood_name,
                                return_results=False,
                                save_dir=save_dir_)

    def _filter_ood_groups(self, ood_groups_selections: Optional[list]):
        if ood_groups_selections is not None:
            assert all([ood_group_name in self.out_domain_scores for ood_group_name in ood_groups_selections]), \
                &#34;Please these OOD groups first before calculating AUC-ROC scores&#34;
            selected_ood_group = self.out_domain_scores.keys() &amp; ood_groups_selections
        else:
            selected_ood_group = self.out_domain_scores.keys()

        return selected_ood_group

    def _score_in_domain(self,
                         X_test: pd.DataFrame):
        &#34;&#34;&#34;
        Returns novelty scores for each sample in the dataset.
        &#34;&#34;&#34;
        scores = score_dataset(X=X_test, models_trials_dict=self.novelty_estimators)
        return scores

    def _score_out_domain(self,
                          ood_groups: dict):
        &#34;&#34;&#34;
        Returns novelty scores for each sample in all OOD groups.
        &#34;&#34;&#34;
        out_domain_scores = dict()

        print(&#34;Scoring OOD data:&#34;)
        for ood_group_name, X_ood in ood_groups.items():
            print(f&#34;\t{ood_group_name}...&#34;, end=&#34; &#34;)
            X_ood = check_and_convert_dfs_to_numpy([X_ood])[0]
            scores = score_dataset(X=X_ood, models_trials_dict=self.novelty_estimators)
            out_domain_scores[ood_group_name] = scores
            print(&#34;done.&#34;)

        return out_domain_scores</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline"><code class="flex name class">
<span>class <span class="ident">OODPipeline</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Pipeline to fit novelty estimators on in-distribution data and evaluate novelty of Out-of-Distribution (OOD)
groups.</p>
<p>Example of usage:</p>
<pre><code># Initialize the pipeline
oodpipeline = OODPipeline()

# Fit the pipeline on in-distribution training data and compute novelty scores for in-distribution test data
oodpipeline.fit(X_train= X_train, X_test=X_test)

# Define OOD groups and evaluate by the pipeline
ood_groups = {"Flu patients": X_flu, "Ventilated patients": X_vent}
oodpipeline.evaluate(ood_groups)

# Inspect AUC-ROC scores of detecting OOD groups
oodpipeline.get_ood_aucs_scores()
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>kwargs</code></strong></dt>
<dd>model_selection: set
Define which models to train, e.g. {"PPCA", "LOF", "VAE"}. If selection is not provided, all available
models are used.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class OODPipeline(BasePipeline):
    &#34;&#34;&#34;
    Pipeline to fit novelty estimators on in-distribution data and evaluate novelty of Out-of-Distribution (OOD)
    groups.


    Example of usage:

        # Initialize the pipeline
        oodpipeline = OODPipeline()

        # Fit the pipeline on in-distribution training data and compute novelty scores for in-distribution test data
        oodpipeline.fit(X_train= X_train, X_test=X_test)

        # Define OOD groups and evaluate by the pipeline
        ood_groups = {&#34;Flu patients&#34;: X_flu, &#34;Ventilated patients&#34;: X_vent}
        oodpipeline.evaluate(ood_groups)

        # Inspect AUC-ROC scores of detecting OOD groups
        oodpipeline.get_ood_aucs_scores()
    &#34;&#34;&#34;

    def __init__(self,
                 **kwargs):
        &#34;&#34;&#34;

        Parameters
        ----------
        kwargs
            model_selection: set
                Define which models to train, e.g. {&#34;PPCA&#34;, &#34;LOF&#34;, &#34;VAE&#34;}. If selection is not provided, all available
                models are used.
        &#34;&#34;&#34;
        super().__init__(**kwargs)
        self.in_domain_scores = defaultdict(dict)
        self.out_domain_scores = defaultdict(dict)
        self.feature_names = None

    def fit(self,
            X_train,
            X_test,
            **kwargs):
        &#34;&#34;&#34;
        Fits models on training data with n_trials different runs.  Novelty estimators from each run are stored
        in a nested dictionary in self.novelty_estimators.
        (E.g.: {0: {&#34;AE&#34;: NoveltyEstimator, &#34;PPCA&#34;: NoveltyEstimator},
               1: {&#34;AE&#34;: NoveltyEstimator, &#34;PPCA&#34;: NoveltyEstimator}} )
        Parameters
        ----------
        X_train: pd.DataFrame
            Training in-distribution data. Used to fit novelty estimators.
        X_test: pd.DataFrame
            Test in-distribution data. Used to calculate self.in_domain_scores which are taken as base novelty scores
            for the dataset and used for comparison against OOD groups later.
        kwargs:
            y_train: pd.DataFrame
                Labels corresponding to training data.
            n_trials: int
                Number of trials to run.
        &#34;&#34;&#34;
        y_train = kwargs.get(&#34;y_train&#34;, None)
        n_trials = kwargs.get(&#34;n_trials&#34;, 5)

        assert list(X_train.columns) == list(X_test.columns), &#34;Train and test data have different features!&#34;
        self.feature_names = list(X_train.columns)

        print(&#34;--- OOD Pipeline ---&#34;)
        print(&#34;1/2 Fitting novelty estimators...&#34;)
        self._fit(X_train=X_train, y_train=y_train, n_trials=n_trials)

        print(&#34;2/2 Scoring in-domain data...&#34;)
        self.in_domain_scores = self._score_in_domain(X_test)

    def evaluate_ood_groups(self,
                            ood_groups: dict,
                            return_averaged: bool = False):
        &#34;&#34;&#34;
        Gives novelty scores to OOD groups.
        Returns and stores dictionary of novelty scores given by each model for each sample in every OOD group.
        If the function is called repeadetly, updates internally stored novelty scores for the OOD groups.

        Parameters
        ----------
        ood_groups: dict
            Dictionary of OOD groups. Dictionary has to contain a name of each OOD group and features in a pd.DataFrame.
            Example: {&#34;Flu patients&#34;: X_flu, &#34;Ventilated patients&#34;: X_vent}

        return_averaged: bool
            If true, returns averaged novelty score for each sample. The shape of novelty scores given by each model
            then corresponds to (1, n_samples).
            Else, the shape of novelty scores given by each model is (n_trials, n_samples) where n_trials is
            the number of trials used in the fit function.

        Returns
        -------
        out_domain_scores: dict
            Returns a dictionary of novelty scores given by each model for each sample in every OOD group.

        &#34;&#34;&#34;
        assert self.novelty_estimators, &#34;Novelty estimator dictionary is empty.&#34; \
                                        &#34;Please fit novelty estimators to in-distribution data before calling&#34; \
                                        &#34;evaluate_ood_groups.&#34;

        assert all([list(X_ood.columns) == self.feature_names for _, X_ood in ood_groups.items()]), \
            &#34;All OOD groups must have identical features to in-distribution data!&#34;

        out_domain_scores = self._score_out_domain(ood_groups)
        self.out_domain_scores.update(out_domain_scores)

        if return_averaged:
            return average_values_in_nested_dict(out_domain_scores)

        return out_domain_scores

    def get_auc_scores(self,
                       ood_groups_selections: Optional[list] = None,
                       return_averaged: bool = True):
        &#34;&#34;&#34;
        Computes AUC-ROC scores of OOD detection for each OOD group as compared to the in-distribution test data.
        By default, returns scores for every group evaluated by the pipeline (evaluate_ood_groups).

        Parameters
        ----------
        ood_groups_selections: Optional(list)
            Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
            is provided, all groups ever evaluate by the pipeline will be included.
        return_averaged: bool
            Indicates whether to return averaged AUC-ROC scores over n_trials run or a list of scores for every trial.

        Returns
        -------
        aucs_dict_groups: dict
            A nested dictionary that contains a name of OOD group, name of novelty estimator and either a float (if
            averaged) or a list of AUC-ROC scores.

        &#34;&#34;&#34;
        selected_ood_group = self._filter_ood_groups(ood_groups_selections)
        aucs_dict_groups = defaultdict(lambda: defaultdict(list))

        for ood_group_name in selected_ood_group:
            aucs_dict_groups[ood_group_name] = get_ood_aucs_score_for_all_models(
                ood_scores_trials_dict=self.out_domain_scores[ood_group_name],
                test_scores_trials_dict=self.in_domain_scores,
            )

        if return_averaged:
            return average_values_in_nested_dict(aucs_dict_groups)

        return aucs_dict_groups

    def plot_auc_scores(self,
                        ood_groups_selections: Optional[list] = None,
                        show_stderr: bool = True,
                        save_dir: str = None,
                        **plot_kwargs):
        &#34;&#34;&#34;
        Plots a heatmap of AUC-ROC scores of OOD detection for each OOD group as compared to
        the in-distribution test data.

        Parameters
        ----------
        ood_groups_selections: Optional(list)
            Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
            is provided, all groups ever evaluate by the pipeline will be included.
        show_stderr: Optional(bool)
            If True (default), annotates the heatmpa with means and standard error (calculated using jacknife
            resampling). Else, plots the mean values only.
        save_dir: Optional(str)
            If a path to a directory is provided, saves plots for each OOD group separately.
        plot_kwargs:
            Other arguments to be passed to sns.heatmap function.
        &#34;&#34;&#34;

        auc_scores = self.get_auc_scores(ood_groups_selections=ood_groups_selections,
                                         return_averaged=show_stderr)
        if not show_stderr:
            annots = get_mean_stderr_annots_in_nested_dict(auc_scores)
            annots = pd.DataFrame(annots).values.T
            auc_scores = average_values_in_nested_dict(auc_scores)
            plot_fmt = &#34;s&#34;

        else:
            annots = True
            plot_fmt = &#34;.2g&#34;

        plot_df = pd.DataFrame(auc_scores)

        plot_heatmap(plot_df,
                     title=&#34;AUC&#34;,
                     save_dir=save_dir,
                     annot=annots,
                     fmt=plot_fmt,
                     annot_kws={&#34;fontsize&#34;: 9},
                     figsize=(12, 0.75 * len(plot_df.columns)),
                     **plot_kwargs,
                     )

    def plot_score_distr(self,
                         ood_groups_selections: Optional[list] = None,
                         save_dir=None
                         ):
        &#34;&#34;&#34;

        Parameters
        ----------
        ood_groups_selections: Optional(list)
            Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
            is provided, all groups ever evaluate by the pipeline will be included.
        save_dir: Optional(str)
            If a path to a directory is provided, saves plots for each OOD group separately.
        &#34;&#34;&#34;
        out_domain_scores_mean = average_values_in_nested_dict(self.out_domain_scores, axis=0, dict_level=2)
        in_domain_scores_mean = average_values_in_nested_dict(self.in_domain_scores, axis=0, dict_level=1)

        selected_ood_group = self._filter_ood_groups(ood_groups_selections)

        for ood_name in selected_ood_group:
            ood_scores = out_domain_scores_mean[ood_name]
            save_group_name = ood_name.lower().replace(&#34; &#34;, &#34;_&#34;)

            if save_dir is not None:
                save_dir_ = f&#34;{save_dir}_{save_group_name}.png&#34;
            else:
                save_dir_ = None

            plot_scores_distr(scores_test=in_domain_scores_mean,
                              scores_new=ood_scores,
                              title=ood_name,
                              clip_q=0.05,
                              kind=&#34;hist&#34;,
                              bins=30,
                              save_dir=save_dir_,
                              labels=SCORING_FUNCTIONS)

    def plot_box_plot(self,
                      ood_groups_selections: Optional[list] = None,
                      save_dir=None
                      ):
        &#34;&#34;&#34;

        Parameters
        ----------
        ood_groups_selections: Optional(list)
            Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
            is provided, all groups ever evaluate by the pipeline will be included.
        save_dir: Optional(str)
            If a path to a directory is provided, saves plots for each OOD group separately.
        &#34;&#34;&#34;

        out_domain_scores_mean = average_values_in_nested_dict(self.out_domain_scores, axis=0, dict_level=2)
        in_domain_scores_mean = average_values_in_nested_dict(self.in_domain_scores, axis=0, dict_level=1)

        selected_ood_group = self._filter_ood_groups(ood_groups_selections)

        for ood_name in selected_ood_group:
            ood_scores = out_domain_scores_mean[ood_name]

            if save_dir is not None:
                save_dir_ = f&#34;{save_dir}_{ood_name}.png&#34;
            else:
                save_dir_ = None

            plot_scores_boxplot(scores_test=in_domain_scores_mean,
                                scores_new=ood_scores,
                                show_outliers=True,
                                title=ood_name,
                                return_results=False,
                                save_dir=save_dir_)

    def _filter_ood_groups(self, ood_groups_selections: Optional[list]):
        if ood_groups_selections is not None:
            assert all([ood_group_name in self.out_domain_scores for ood_group_name in ood_groups_selections]), \
                &#34;Please these OOD groups first before calculating AUC-ROC scores&#34;
            selected_ood_group = self.out_domain_scores.keys() &amp; ood_groups_selections
        else:
            selected_ood_group = self.out_domain_scores.keys()

        return selected_ood_group

    def _score_in_domain(self,
                         X_test: pd.DataFrame):
        &#34;&#34;&#34;
        Returns novelty scores for each sample in the dataset.
        &#34;&#34;&#34;
        scores = score_dataset(X=X_test, models_trials_dict=self.novelty_estimators)
        return scores

    def _score_out_domain(self,
                          ood_groups: dict):
        &#34;&#34;&#34;
        Returns novelty scores for each sample in all OOD groups.
        &#34;&#34;&#34;
        out_domain_scores = dict()

        print(&#34;Scoring OOD data:&#34;)
        for ood_group_name, X_ood in ood_groups.items():
            print(f&#34;\t{ood_group_name}...&#34;, end=&#34; &#34;)
            X_ood = check_and_convert_dfs_to_numpy([X_ood])[0]
            scores = score_dataset(X=X_ood, models_trials_dict=self.novelty_estimators)
            out_domain_scores[ood_group_name] = scores
            print(&#34;done.&#34;)

        return out_domain_scores</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="selecting_OOD_detector.pipeline.base.BasePipeline" href="base.html#selecting_OOD_detector.pipeline.base.BasePipeline">BasePipeline</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.evaluate_ood_groups"><code class="name flex">
<span>def <span class="ident">evaluate_ood_groups</span></span>(<span>self, ood_groups: dict, return_averaged: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Gives novelty scores to OOD groups.
Returns and stores dictionary of novelty scores given by each model for each sample in every OOD group.
If the function is called repeadetly, updates internally stored novelty scores for the OOD groups.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ood_groups</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary of OOD groups. Dictionary has to contain a name of each OOD group and features in a pd.DataFrame.
Example: {"Flu patients": X_flu, "Ventilated patients": X_vent}</dd>
<dt><strong><code>return_averaged</code></strong> :&ensp;<code>bool</code></dt>
<dd>If true, returns averaged novelty score for each sample. The shape of novelty scores given by each model
then corresponds to (1, n_samples).
Else, the shape of novelty scores given by each model is (n_trials, n_samples) where n_trials is
the number of trials used in the fit function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out_domain_scores</code></strong> :&ensp;<code>dict</code></dt>
<dd>Returns a dictionary of novelty scores given by each model for each sample in every OOD group.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_ood_groups(self,
                        ood_groups: dict,
                        return_averaged: bool = False):
    &#34;&#34;&#34;
    Gives novelty scores to OOD groups.
    Returns and stores dictionary of novelty scores given by each model for each sample in every OOD group.
    If the function is called repeadetly, updates internally stored novelty scores for the OOD groups.

    Parameters
    ----------
    ood_groups: dict
        Dictionary of OOD groups. Dictionary has to contain a name of each OOD group and features in a pd.DataFrame.
        Example: {&#34;Flu patients&#34;: X_flu, &#34;Ventilated patients&#34;: X_vent}

    return_averaged: bool
        If true, returns averaged novelty score for each sample. The shape of novelty scores given by each model
        then corresponds to (1, n_samples).
        Else, the shape of novelty scores given by each model is (n_trials, n_samples) where n_trials is
        the number of trials used in the fit function.

    Returns
    -------
    out_domain_scores: dict
        Returns a dictionary of novelty scores given by each model for each sample in every OOD group.

    &#34;&#34;&#34;
    assert self.novelty_estimators, &#34;Novelty estimator dictionary is empty.&#34; \
                                    &#34;Please fit novelty estimators to in-distribution data before calling&#34; \
                                    &#34;evaluate_ood_groups.&#34;

    assert all([list(X_ood.columns) == self.feature_names for _, X_ood in ood_groups.items()]), \
        &#34;All OOD groups must have identical features to in-distribution data!&#34;

    out_domain_scores = self._score_out_domain(ood_groups)
    self.out_domain_scores.update(out_domain_scores)

    if return_averaged:
        return average_values_in_nested_dict(out_domain_scores)

    return out_domain_scores</code></pre>
</details>
</dd>
<dt id="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X_train, X_test, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Fits models on training data with n_trials different runs.
Novelty estimators from each run are stored
in a nested dictionary in self.novelty_estimators.
(E.g.: {0: {"AE": NoveltyEstimator, "PPCA": NoveltyEstimator},
1: {"AE": NoveltyEstimator, "PPCA": NoveltyEstimator}} )
Parameters</p>
<hr>
<dl>
<dt><strong><code>X_train</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Training in-distribution data. Used to fit novelty estimators.</dd>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Test in-distribution data. Used to calculate self.in_domain_scores which are taken as base novelty scores
for the dataset and used for comparison against OOD groups later.</dd>
</dl>
<p>kwargs:
y_train: pd.DataFrame
Labels corresponding to training data.
n_trials: int
Number of trials to run.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self,
        X_train,
        X_test,
        **kwargs):
    &#34;&#34;&#34;
    Fits models on training data with n_trials different runs.  Novelty estimators from each run are stored
    in a nested dictionary in self.novelty_estimators.
    (E.g.: {0: {&#34;AE&#34;: NoveltyEstimator, &#34;PPCA&#34;: NoveltyEstimator},
           1: {&#34;AE&#34;: NoveltyEstimator, &#34;PPCA&#34;: NoveltyEstimator}} )
    Parameters
    ----------
    X_train: pd.DataFrame
        Training in-distribution data. Used to fit novelty estimators.
    X_test: pd.DataFrame
        Test in-distribution data. Used to calculate self.in_domain_scores which are taken as base novelty scores
        for the dataset and used for comparison against OOD groups later.
    kwargs:
        y_train: pd.DataFrame
            Labels corresponding to training data.
        n_trials: int
            Number of trials to run.
    &#34;&#34;&#34;
    y_train = kwargs.get(&#34;y_train&#34;, None)
    n_trials = kwargs.get(&#34;n_trials&#34;, 5)

    assert list(X_train.columns) == list(X_test.columns), &#34;Train and test data have different features!&#34;
    self.feature_names = list(X_train.columns)

    print(&#34;--- OOD Pipeline ---&#34;)
    print(&#34;1/2 Fitting novelty estimators...&#34;)
    self._fit(X_train=X_train, y_train=y_train, n_trials=n_trials)

    print(&#34;2/2 Scoring in-domain data...&#34;)
    self.in_domain_scores = self._score_in_domain(X_test)</code></pre>
</details>
</dd>
<dt id="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.get_auc_scores"><code class="name flex">
<span>def <span class="ident">get_auc_scores</span></span>(<span>self, ood_groups_selections: Optional[list] = None, return_averaged: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes AUC-ROC scores of OOD detection for each OOD group as compared to the in-distribution test data.
By default, returns scores for every group evaluated by the pipeline (evaluate_ood_groups).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ood_groups_selections</code></strong> :&ensp;<code>Optional(list)</code></dt>
<dd>Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
is provided, all groups ever evaluate by the pipeline will be included.</dd>
<dt><strong><code>return_averaged</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicates whether to return averaged AUC-ROC scores over n_trials run or a list of scores for every trial.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>aucs_dict_groups</code></strong> :&ensp;<code>dict</code></dt>
<dd>A nested dictionary that contains a name of OOD group, name of novelty estimator and either a float (if
averaged) or a list of AUC-ROC scores.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_auc_scores(self,
                   ood_groups_selections: Optional[list] = None,
                   return_averaged: bool = True):
    &#34;&#34;&#34;
    Computes AUC-ROC scores of OOD detection for each OOD group as compared to the in-distribution test data.
    By default, returns scores for every group evaluated by the pipeline (evaluate_ood_groups).

    Parameters
    ----------
    ood_groups_selections: Optional(list)
        Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
        is provided, all groups ever evaluate by the pipeline will be included.
    return_averaged: bool
        Indicates whether to return averaged AUC-ROC scores over n_trials run or a list of scores for every trial.

    Returns
    -------
    aucs_dict_groups: dict
        A nested dictionary that contains a name of OOD group, name of novelty estimator and either a float (if
        averaged) or a list of AUC-ROC scores.

    &#34;&#34;&#34;
    selected_ood_group = self._filter_ood_groups(ood_groups_selections)
    aucs_dict_groups = defaultdict(lambda: defaultdict(list))

    for ood_group_name in selected_ood_group:
        aucs_dict_groups[ood_group_name] = get_ood_aucs_score_for_all_models(
            ood_scores_trials_dict=self.out_domain_scores[ood_group_name],
            test_scores_trials_dict=self.in_domain_scores,
        )

    if return_averaged:
        return average_values_in_nested_dict(aucs_dict_groups)

    return aucs_dict_groups</code></pre>
</details>
</dd>
<dt id="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.plot_auc_scores"><code class="name flex">
<span>def <span class="ident">plot_auc_scores</span></span>(<span>self, ood_groups_selections: Optional[list] = None, show_stderr: bool = True, save_dir: str = None, **plot_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Plots a heatmap of AUC-ROC scores of OOD detection for each OOD group as compared to
the in-distribution test data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ood_groups_selections</code></strong> :&ensp;<code>Optional(list)</code></dt>
<dd>Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
is provided, all groups ever evaluate by the pipeline will be included.</dd>
<dt><strong><code>show_stderr</code></strong> :&ensp;<code>Optional(bool)</code></dt>
<dd>If True (default), annotates the heatmpa with means and standard error (calculated using jacknife
resampling). Else, plots the mean values only.</dd>
<dt><strong><code>save_dir</code></strong> :&ensp;<code>Optional(str)</code></dt>
<dd>If a path to a directory is provided, saves plots for each OOD group separately.</dd>
</dl>
<p>plot_kwargs:
Other arguments to be passed to sns.heatmap function.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_auc_scores(self,
                    ood_groups_selections: Optional[list] = None,
                    show_stderr: bool = True,
                    save_dir: str = None,
                    **plot_kwargs):
    &#34;&#34;&#34;
    Plots a heatmap of AUC-ROC scores of OOD detection for each OOD group as compared to
    the in-distribution test data.

    Parameters
    ----------
    ood_groups_selections: Optional(list)
        Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
        is provided, all groups ever evaluate by the pipeline will be included.
    show_stderr: Optional(bool)
        If True (default), annotates the heatmpa with means and standard error (calculated using jacknife
        resampling). Else, plots the mean values only.
    save_dir: Optional(str)
        If a path to a directory is provided, saves plots for each OOD group separately.
    plot_kwargs:
        Other arguments to be passed to sns.heatmap function.
    &#34;&#34;&#34;

    auc_scores = self.get_auc_scores(ood_groups_selections=ood_groups_selections,
                                     return_averaged=show_stderr)
    if not show_stderr:
        annots = get_mean_stderr_annots_in_nested_dict(auc_scores)
        annots = pd.DataFrame(annots).values.T
        auc_scores = average_values_in_nested_dict(auc_scores)
        plot_fmt = &#34;s&#34;

    else:
        annots = True
        plot_fmt = &#34;.2g&#34;

    plot_df = pd.DataFrame(auc_scores)

    plot_heatmap(plot_df,
                 title=&#34;AUC&#34;,
                 save_dir=save_dir,
                 annot=annots,
                 fmt=plot_fmt,
                 annot_kws={&#34;fontsize&#34;: 9},
                 figsize=(12, 0.75 * len(plot_df.columns)),
                 **plot_kwargs,
                 )</code></pre>
</details>
</dd>
<dt id="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.plot_box_plot"><code class="name flex">
<span>def <span class="ident">plot_box_plot</span></span>(<span>self, ood_groups_selections: Optional[list] = None, save_dir=None)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ood_groups_selections</code></strong> :&ensp;<code>Optional(list)</code></dt>
<dd>Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
is provided, all groups ever evaluate by the pipeline will be included.</dd>
<dt><strong><code>save_dir</code></strong> :&ensp;<code>Optional(str)</code></dt>
<dd>If a path to a directory is provided, saves plots for each OOD group separately.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_box_plot(self,
                  ood_groups_selections: Optional[list] = None,
                  save_dir=None
                  ):
    &#34;&#34;&#34;

    Parameters
    ----------
    ood_groups_selections: Optional(list)
        Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
        is provided, all groups ever evaluate by the pipeline will be included.
    save_dir: Optional(str)
        If a path to a directory is provided, saves plots for each OOD group separately.
    &#34;&#34;&#34;

    out_domain_scores_mean = average_values_in_nested_dict(self.out_domain_scores, axis=0, dict_level=2)
    in_domain_scores_mean = average_values_in_nested_dict(self.in_domain_scores, axis=0, dict_level=1)

    selected_ood_group = self._filter_ood_groups(ood_groups_selections)

    for ood_name in selected_ood_group:
        ood_scores = out_domain_scores_mean[ood_name]

        if save_dir is not None:
            save_dir_ = f&#34;{save_dir}_{ood_name}.png&#34;
        else:
            save_dir_ = None

        plot_scores_boxplot(scores_test=in_domain_scores_mean,
                            scores_new=ood_scores,
                            show_outliers=True,
                            title=ood_name,
                            return_results=False,
                            save_dir=save_dir_)</code></pre>
</details>
</dd>
<dt id="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.plot_score_distr"><code class="name flex">
<span>def <span class="ident">plot_score_distr</span></span>(<span>self, ood_groups_selections: Optional[list] = None, save_dir=None)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ood_groups_selections</code></strong> :&ensp;<code>Optional(list)</code></dt>
<dd>Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
is provided, all groups ever evaluate by the pipeline will be included.</dd>
<dt><strong><code>save_dir</code></strong> :&ensp;<code>Optional(str)</code></dt>
<dd>If a path to a directory is provided, saves plots for each OOD group separately.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_score_distr(self,
                     ood_groups_selections: Optional[list] = None,
                     save_dir=None
                     ):
    &#34;&#34;&#34;

    Parameters
    ----------
    ood_groups_selections: Optional(list)
        Optionally provide a selection of OOD groups for which AUC-ROC score should be returned. If no selection
        is provided, all groups ever evaluate by the pipeline will be included.
    save_dir: Optional(str)
        If a path to a directory is provided, saves plots for each OOD group separately.
    &#34;&#34;&#34;
    out_domain_scores_mean = average_values_in_nested_dict(self.out_domain_scores, axis=0, dict_level=2)
    in_domain_scores_mean = average_values_in_nested_dict(self.in_domain_scores, axis=0, dict_level=1)

    selected_ood_group = self._filter_ood_groups(ood_groups_selections)

    for ood_name in selected_ood_group:
        ood_scores = out_domain_scores_mean[ood_name]
        save_group_name = ood_name.lower().replace(&#34; &#34;, &#34;_&#34;)

        if save_dir is not None:
            save_dir_ = f&#34;{save_dir}_{save_group_name}.png&#34;
        else:
            save_dir_ = None

        plot_scores_distr(scores_test=in_domain_scores_mean,
                          scores_new=ood_scores,
                          title=ood_name,
                          clip_q=0.05,
                          kind=&#34;hist&#34;,
                          bins=30,
                          save_dir=save_dir_,
                          labels=SCORING_FUNCTIONS)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="selecting_OOD_detector.pipeline" href="index.html">selecting_OOD_detector.pipeline</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline" href="#selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline">OODPipeline</a></code></h4>
<ul class="two-column">
<li><code><a title="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.evaluate_ood_groups" href="#selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.evaluate_ood_groups">evaluate_ood_groups</a></code></li>
<li><code><a title="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.fit" href="#selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.fit">fit</a></code></li>
<li><code><a title="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.get_auc_scores" href="#selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.get_auc_scores">get_auc_scores</a></code></li>
<li><code><a title="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.plot_auc_scores" href="#selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.plot_auc_scores">plot_auc_scores</a></code></li>
<li><code><a title="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.plot_box_plot" href="#selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.plot_box_plot">plot_box_plot</a></code></li>
<li><code><a title="selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.plot_score_distr" href="#selecting_OOD_detector.pipeline.ood_pipeline.OODPipeline.plot_score_distr">plot_score_distr</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>